{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning of the Google Store Analytics Dataset\n",
    "\n",
    "## Random Forest Regressor Models - v3\n",
    "### Version 3: Limit Input Variables, Label Encoding, No Scaling/Transforming of Data\n",
    "\n",
    "This dataset is provided by the Kaggle competition.  \n",
    "https://www.kaggle.com/c/ga-customer-revenue-prediction\n",
    "\n",
    "We performed some data engineering and datetime feature engineering to get the dataset to the state we wanted.\n",
    "\n",
    "Now we will try a variety of different models and look at their accuracy.  The models we will try:\n",
    "1. Generalized Linear Regression Models\n",
    "    1. Linear Regression (Ordinary Least Squares) Model\n",
    "    2. Linear Lasso Regression Model\n",
    "    3. Linear Ridge Regression Model\n",
    "    4. Linear Elastic Net Regression Model\n",
    "2. Decision Tree Regression - a combination of decision trees and getting continuous data output http://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html  http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor\n",
    "3. Random Forest Regression??\n",
    "4. Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, confusion_matrix\n",
    "\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "from graphviz import Source\n",
    "from subprocess import call\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and Pre-processing of the Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('data/train_v1_full_data_split.pkl', 'rb') as fp:\n",
    "#     df = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(903652, 44)\n"
     ]
    }
   ],
   "source": [
    "#import the data engineered and feature engineered training dataset\n",
    "df = pd.read_pickle('/home/michael_suomi/Final-Project-Google-Merch-Store/data/train_v1_full_data_split.pkl')\n",
    "print(df.shape)\n",
    "# print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(903652, 43)\n"
     ]
    }
   ],
   "source": [
    "### DROP COLUMNS NOT IN FINAL TEST DATA ###\n",
    "#the test dataset does not have the 'trafficSource_campaignCode' column, so drop that from our training set too\n",
    "df.drop('trafficSource_campaignCode', axis=1, inplace=True)\n",
    "print(df.shape)\n",
    "# print(df.columns)\n",
    "# df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### CHANGE TRANSACTION REVENUE FROM NANs to 0 AND CHANGE to FLOAT TYPE (some are strings)###\n",
    "df.totals_transactionRevenue.fillna(0, inplace=True)\n",
    "df.totals_transactionRevenue = df.totals_transactionRevenue.astype(dtype=float)\n",
    "\n",
    "### CHANGE OTHER STRINGS TO INTS/FLOATS WHERE NEEDED ###\n",
    "#stick to floats rather than ints since a np.nan is a float object\n",
    "df.totals_bounces = df.totals_bounces.astype(dtype=float)\n",
    "df.totals_hits = df.totals_hits.astype(dtype=float)\n",
    "df.totals_newVisits = df.totals_newVisits.astype(dtype=float)\n",
    "df.totals_pageviews = df.totals_pageviews.astype(dtype=float)\n",
    "df.totals_visits = df.totals_visits.astype(dtype=float)\n",
    "\n",
    "### CONVERT NANs in bounces, newVisits to 0 values ###\n",
    "#the blank NAN values for these columns imply a 0 value meaning 0 newVisits or 0 bounces\n",
    "df.totals_bounces.fillna(0, inplace=True)\n",
    "df.totals_newVisits.fillna(0, inplace=True)\n",
    "# df.totals_visits.fillna(0, inplace=True) #there shouldn't be anyone with 0 visits (they've at least visited once or woulnd't be recorded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### REVENUE IS DOLLARS * 10^6, NOT EXPONENTIAL LIKE WE THOUGHT ####\n",
    "#### SINCE THE REVENUE IS SCALED UP BY A CONSTANT, NO NEED TO ADJUST FOR LIN REGRESS MODEL ####\n",
    "# ### CONVERT TRANSACTION REVENUE TO DOLLARS (instead of the e^dollars_revenue) ###\n",
    "# df['totals_transactionRevenue_dollars'] = df.totals_transactionRevenue.map(lambda x:\n",
    "#                                                                             np.log1p(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(903652, 43)\n",
      "Index(['channelGrouping', 'date', 'fullVisitorId', 'sessionId',\n",
      "       'socialEngagementType', 'visitId', 'visitNumber', 'visitStartTime',\n",
      "       'device_deviceCategory', 'device_browser', 'device_isMobile',\n",
      "       'device_operatingSystem', 'geoNetwork_subContinent',\n",
      "       'geoNetwork_region', 'geoNetwork_continent', 'geoNetwork_country',\n",
      "       'geoNetwork_city', 'geoNetwork_metro', 'geoNetwork_networkDomain',\n",
      "       'totals_bounces', 'totals_hits', 'totals_newVisits', 'totals_pageviews',\n",
      "       'totals_visits', 'totals_transactionRevenue',\n",
      "       'trafficSource_isTrueDirect', 'trafficSource_keyword',\n",
      "       'trafficSource_source', 'trafficSource_adContent',\n",
      "       'trafficSource_medium', 'trafficSource_referralPath',\n",
      "       'trafficSource_campaign', 'city_country', 'lat_lng', 'timezone',\n",
      "       'datetime_iso_utc', 'datetime_iso_local', 'year_local', 'month_local',\n",
      "       'day_local', 'yearday_local', 'weekday_local', 'hour_local'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channelGrouping</th>\n",
       "      <th>date</th>\n",
       "      <th>fullVisitorId</th>\n",
       "      <th>sessionId</th>\n",
       "      <th>socialEngagementType</th>\n",
       "      <th>visitId</th>\n",
       "      <th>visitNumber</th>\n",
       "      <th>visitStartTime</th>\n",
       "      <th>device_deviceCategory</th>\n",
       "      <th>device_browser</th>\n",
       "      <th>...</th>\n",
       "      <th>lat_lng</th>\n",
       "      <th>timezone</th>\n",
       "      <th>datetime_iso_utc</th>\n",
       "      <th>datetime_iso_local</th>\n",
       "      <th>year_local</th>\n",
       "      <th>month_local</th>\n",
       "      <th>day_local</th>\n",
       "      <th>yearday_local</th>\n",
       "      <th>weekday_local</th>\n",
       "      <th>hour_local</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Organic Search</td>\n",
       "      <td>20160902</td>\n",
       "      <td>1131660440785968503</td>\n",
       "      <td>1131660440785968503_1472830385</td>\n",
       "      <td>Not Socially Engaged</td>\n",
       "      <td>1472830385</td>\n",
       "      <td>1</td>\n",
       "      <td>1472830385</td>\n",
       "      <td>desktop</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>...</td>\n",
       "      <td>(38.423734, 27.142826)</td>\n",
       "      <td>(+03, 3.0)</td>\n",
       "      <td>2016-09-02 15:33:05+00:00</td>\n",
       "      <td>2016-09-02 18:33:05+03:00</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>246.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Organic Search</td>\n",
       "      <td>20160902</td>\n",
       "      <td>377306020877927890</td>\n",
       "      <td>377306020877927890_1472880147</td>\n",
       "      <td>Not Socially Engaged</td>\n",
       "      <td>1472880147</td>\n",
       "      <td>1</td>\n",
       "      <td>1472880147</td>\n",
       "      <td>desktop</td>\n",
       "      <td>Firefox</td>\n",
       "      <td>...</td>\n",
       "      <td>(-25.274398, 133.775136)</td>\n",
       "      <td>(ACST, 9.5)</td>\n",
       "      <td>2016-09-03 05:22:27+00:00</td>\n",
       "      <td>2016-09-03 14:52:27+09:30</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>247.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Organic Search</td>\n",
       "      <td>20160902</td>\n",
       "      <td>3895546263509774583</td>\n",
       "      <td>3895546263509774583_1472865386</td>\n",
       "      <td>Not Socially Engaged</td>\n",
       "      <td>1472865386</td>\n",
       "      <td>1</td>\n",
       "      <td>1472865386</td>\n",
       "      <td>desktop</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>...</td>\n",
       "      <td>(40.4167754, -3.7037902)</td>\n",
       "      <td>(CEST, 2.0)</td>\n",
       "      <td>2016-09-03 01:16:26+00:00</td>\n",
       "      <td>2016-09-03 03:16:26+02:00</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>247.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  channelGrouping      date        fullVisitorId  \\\n",
       "0  Organic Search  20160902  1131660440785968503   \n",
       "1  Organic Search  20160902   377306020877927890   \n",
       "2  Organic Search  20160902  3895546263509774583   \n",
       "\n",
       "                        sessionId  socialEngagementType     visitId  \\\n",
       "0  1131660440785968503_1472830385  Not Socially Engaged  1472830385   \n",
       "1   377306020877927890_1472880147  Not Socially Engaged  1472880147   \n",
       "2  3895546263509774583_1472865386  Not Socially Engaged  1472865386   \n",
       "\n",
       "   visitNumber  visitStartTime device_deviceCategory device_browser  \\\n",
       "0            1      1472830385               desktop         Chrome   \n",
       "1            1      1472880147               desktop        Firefox   \n",
       "2            1      1472865386               desktop         Chrome   \n",
       "\n",
       "      ...                       lat_lng     timezone  \\\n",
       "0     ...        (38.423734, 27.142826)   (+03, 3.0)   \n",
       "1     ...      (-25.274398, 133.775136)  (ACST, 9.5)   \n",
       "2     ...      (40.4167754, -3.7037902)  (CEST, 2.0)   \n",
       "\n",
       "            datetime_iso_utc         datetime_iso_local year_local  \\\n",
       "0  2016-09-02 15:33:05+00:00  2016-09-02 18:33:05+03:00     2016.0   \n",
       "1  2016-09-03 05:22:27+00:00  2016-09-03 14:52:27+09:30     2016.0   \n",
       "2  2016-09-03 01:16:26+00:00  2016-09-03 03:16:26+02:00     2016.0   \n",
       "\n",
       "  month_local day_local yearday_local weekday_local  hour_local  \n",
       "0         9.0       2.0         246.0           5.0        18.0  \n",
       "1         9.0       3.0         247.0           6.0        14.0  \n",
       "2         9.0       3.0         247.0           6.0         3.0  \n",
       "\n",
       "[3 rows x 43 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### VIEW THE DATA BEFORE LABEL ENCODING ###\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>visitId</th>\n",
       "      <th>visitNumber</th>\n",
       "      <th>visitStartTime</th>\n",
       "      <th>totals_bounces</th>\n",
       "      <th>totals_hits</th>\n",
       "      <th>totals_newVisits</th>\n",
       "      <th>totals_pageviews</th>\n",
       "      <th>totals_visits</th>\n",
       "      <th>totals_transactionRevenue</th>\n",
       "      <th>year_local</th>\n",
       "      <th>month_local</th>\n",
       "      <th>day_local</th>\n",
       "      <th>yearday_local</th>\n",
       "      <th>weekday_local</th>\n",
       "      <th>hour_local</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9.036520e+05</td>\n",
       "      <td>9.036520e+05</td>\n",
       "      <td>903652.000000</td>\n",
       "      <td>9.036520e+05</td>\n",
       "      <td>903652.000000</td>\n",
       "      <td>903652.000000</td>\n",
       "      <td>903652.000000</td>\n",
       "      <td>903552.000000</td>\n",
       "      <td>903652.0</td>\n",
       "      <td>9.036520e+05</td>\n",
       "      <td>902175.000000</td>\n",
       "      <td>902175.000000</td>\n",
       "      <td>902175.000000</td>\n",
       "      <td>902175.000000</td>\n",
       "      <td>902175.000000</td>\n",
       "      <td>902175.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.016589e+07</td>\n",
       "      <td>1.485007e+09</td>\n",
       "      <td>2.264898</td>\n",
       "      <td>1.485007e+09</td>\n",
       "      <td>0.498675</td>\n",
       "      <td>4.596542</td>\n",
       "      <td>0.778020</td>\n",
       "      <td>3.849767</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.704275e+06</td>\n",
       "      <td>2016.517473</td>\n",
       "      <td>6.990086</td>\n",
       "      <td>15.698499</td>\n",
       "      <td>197.611083</td>\n",
       "      <td>3.739715</td>\n",
       "      <td>13.898355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.697698e+03</td>\n",
       "      <td>9.022128e+06</td>\n",
       "      <td>9.283740</td>\n",
       "      <td>9.022128e+06</td>\n",
       "      <td>0.499999</td>\n",
       "      <td>9.641442</td>\n",
       "      <td>0.415578</td>\n",
       "      <td>7.025277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.277869e+07</td>\n",
       "      <td>0.499695</td>\n",
       "      <td>3.486402</td>\n",
       "      <td>8.824394</td>\n",
       "      <td>106.757146</td>\n",
       "      <td>1.919636</td>\n",
       "      <td>5.806083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.016080e+07</td>\n",
       "      <td>1.470035e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.470035e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2016.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.016103e+07</td>\n",
       "      <td>1.477561e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.477561e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2016.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.017011e+07</td>\n",
       "      <td>1.483949e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.483949e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2017.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>207.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.017042e+07</td>\n",
       "      <td>1.492759e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.492759e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2017.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>297.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.017080e+07</td>\n",
       "      <td>1.501657e+09</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>1.501657e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>469.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.312950e+10</td>\n",
       "      <td>2017.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>366.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               date       visitId    visitNumber  visitStartTime  \\\n",
       "count  9.036520e+05  9.036520e+05  903652.000000    9.036520e+05   \n",
       "mean   2.016589e+07  1.485007e+09       2.264898    1.485007e+09   \n",
       "std    4.697698e+03  9.022128e+06       9.283740    9.022128e+06   \n",
       "min    2.016080e+07  1.470035e+09       1.000000    1.470035e+09   \n",
       "25%    2.016103e+07  1.477561e+09       1.000000    1.477561e+09   \n",
       "50%    2.017011e+07  1.483949e+09       1.000000    1.483949e+09   \n",
       "75%    2.017042e+07  1.492759e+09       1.000000    1.492759e+09   \n",
       "max    2.017080e+07  1.501657e+09     395.000000    1.501657e+09   \n",
       "\n",
       "       totals_bounces    totals_hits  totals_newVisits  totals_pageviews  \\\n",
       "count   903652.000000  903652.000000     903652.000000     903552.000000   \n",
       "mean         0.498675       4.596542          0.778020          3.849767   \n",
       "std          0.499999       9.641442          0.415578          7.025277   \n",
       "min          0.000000       1.000000          0.000000          1.000000   \n",
       "25%          0.000000       1.000000          1.000000          1.000000   \n",
       "50%          0.000000       2.000000          1.000000          1.000000   \n",
       "75%          1.000000       4.000000          1.000000          4.000000   \n",
       "max          1.000000     500.000000          1.000000        469.000000   \n",
       "\n",
       "       totals_visits  totals_transactionRevenue     year_local    month_local  \\\n",
       "count       903652.0               9.036520e+05  902175.000000  902175.000000   \n",
       "mean             1.0               1.704275e+06    2016.517473       6.990086   \n",
       "std              0.0               5.277869e+07       0.499695       3.486402   \n",
       "min              1.0               0.000000e+00    2016.000000       1.000000   \n",
       "25%              1.0               0.000000e+00    2016.000000       4.000000   \n",
       "50%              1.0               0.000000e+00    2017.000000       7.000000   \n",
       "75%              1.0               0.000000e+00    2017.000000      10.000000   \n",
       "max              1.0               2.312950e+10    2017.000000      12.000000   \n",
       "\n",
       "           day_local  yearday_local  weekday_local     hour_local  \n",
       "count  902175.000000  902175.000000  902175.000000  902175.000000  \n",
       "mean       15.698499     197.611083       3.739715      13.898355  \n",
       "std         8.824394     106.757146       1.919636       5.806083  \n",
       "min         1.000000       1.000000       1.000000       0.000000  \n",
       "25%         8.000000     103.000000       2.000000      10.000000  \n",
       "50%        16.000000     207.000000       4.000000      14.000000  \n",
       "75%        23.000000     297.000000       5.000000      18.000000  \n",
       "max        31.000000     366.000000       7.000000      23.000000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view the numerical data columns for counts, mean, and min/max\n",
    "#if the standard deviation (std) is zero, that means every value is the same - may want to check that data\n",
    "#and see if need to edit it (since describe ignores NANs for instance, you may need to go back and convert the NANs to a \n",
    "#value that makes sense)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataframe Shape:  (903652, 43)\n",
      "\n",
      " Converting Column:  channelGrouping\n",
      "(903652, 43)\n",
      "\n",
      " Converting Column:  socialEngagementType\n",
      "(903652, 43)\n",
      "\n",
      " Converting Column:  device_deviceCategory\n",
      "(903652, 43)\n",
      "\n",
      " Converting Column:  device_browser\n",
      "(903652, 43)\n",
      "\n",
      " Converting Column:  device_isMobile\n",
      "(903652, 43)\n",
      "\n",
      " Converting Column:  device_operatingSystem\n",
      "(903652, 43)\n",
      "\n",
      " Converting Column:  geoNetwork_subContinent\n",
      "(903652, 43)\n",
      "\n",
      " Converting Column:  geoNetwork_region\n",
      "(903652, 43)\n",
      "\n",
      " Converting Column:  geoNetwork_continent\n",
      "(903652, 43)\n",
      "\n",
      " Converting Column:  geoNetwork_country\n",
      "(903652, 43)\n",
      "\n",
      " Converting Column:  geoNetwork_city\n",
      "(903652, 43)\n",
      "\n",
      " Converting Column:  geoNetwork_metro\n",
      "(903652, 43)\n",
      "\n",
      " Converting Column:  geoNetwork_networkDomain\n",
      "(903652, 43)\n",
      "\n",
      " Converting Column:  trafficSource_isTrueDirect\n",
      "(903652, 43)\n",
      "\n",
      " Converting Column:  trafficSource_keyword\n",
      "(903652, 43)\n",
      "\n",
      " Converting Column:  trafficSource_source\n",
      "(903652, 43)\n",
      "\n",
      " Converting Column:  trafficSource_adContent\n",
      "(903652, 43)\n",
      "\n",
      " Converting Column:  trafficSource_medium\n",
      "(903652, 43)\n",
      "\n",
      " Converting Column:  trafficSource_referralPath\n",
      "(903652, 43)\n",
      "\n",
      " Converting Column:  trafficSource_campaign\n",
      "(903652, 43)\n"
     ]
    }
   ],
   "source": [
    "### LABEL ENCODING ALL THE CATEGORICAL VARIABLES ###\n",
    "# label encode the categorical variables\n",
    "categorical_cols = ['channelGrouping', 'socialEngagementType', \n",
    "                   'device_deviceCategory', 'device_browser', 'device_isMobile',\n",
    "                   'device_operatingSystem', 'geoNetwork_subContinent',\n",
    "                   'geoNetwork_region', 'geoNetwork_continent', 'geoNetwork_country',\n",
    "                   'geoNetwork_city', 'geoNetwork_metro', 'geoNetwork_networkDomain',\n",
    "                   'trafficSource_isTrueDirect', 'trafficSource_keyword',\n",
    "                   'trafficSource_source', 'trafficSource_adContent',\n",
    "                   'trafficSource_medium', 'trafficSource_referralPath',\n",
    "                   'trafficSource_campaign']\n",
    "\n",
    "print('Original Dataframe Shape: ', df.shape)\n",
    "\n",
    "for col in categorical_cols:\n",
    "    print('\\n Converting Column: ', col)\n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    lbl.fit(list(df[col].values.astype('str')))\n",
    "    df[col] = lbl.transform(list(df[col].values.astype('str')))\n",
    "    print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decide what Input Data to Use for X and Split Data via train_test_split\n",
    "For initial runs of the models, try using less input data (by using the ones we think are most predictive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ### CALCULATE CORRELATION OF EACH POSSIBLE INPUT vs. REVENUE VALUE ---ORIGINAL DF--- TO HELP DECISION MAKING OF WHICH INPUTS TO INCLUDE###\n",
    "# correlation_summary_list = []\n",
    "\n",
    "# for col in df.columns:\n",
    "#     #can only run correlations on columns that have numerical values (either dtype of float or int)\n",
    "#     #in particular some columns have dtype of 'O', which stands for python object, which in this case means the dtypes are mixed\n",
    "#     if df[col].dtype in ['float64', 'int64']:\n",
    "        \n",
    "#         #having NANs in the dataset for correlations breaks the correlation calculation\n",
    "#         #so only keep the good_rows that don't have nans for either series being used in the correlation calculation\n",
    "#         #and then np.compress(good_rows, series) just reduces the series to the array with only the good_rows to run the correlation\n",
    "#         good_rows = ~np.logical_or(np.isnan(df[col]), np.isnan(df.totals_transactionRevenue))\n",
    "\n",
    "#         #pearsonr function calculates the Pearson correlation coefficient and the p-value for as a tuple\n",
    "#         correl_pvalue = pearsonr(np.compress(good_rows, df[col]), np.compress(good_rows, df.totals_transactionRevenue))\n",
    "\n",
    "#         #create new tuple that also has the column name (which is the input variable) and the correl coef and p-value\n",
    "#         variable_correl_pvalue = (col,) + correl_pvalue\n",
    "\n",
    "#         #add the correl and pvalue tuple to the list of all correlation summaries\n",
    "#         correlation_summary_list.append(variable_correl_pvalue)\n",
    "\n",
    "        \n",
    "# #create a dataframe of the correlation summary (for ease of readibility/manipulation)    \n",
    "# correlation_summary_df = pd.DataFrame(correlation_summary_list, columns=['Input Variable', 'Correlation', 'p-value'])\n",
    "# #create a new column of absolute value of correlations so can easily sort both positive and negative correlations together\n",
    "# correlation_summary_df['Absolute Value Correlation'] = correlation_summary_df.Correlation.map(lambda correl: abs(correl))\n",
    "# #sort the dataframe by absolute value correlation high to low\n",
    "# correlation_summary_df.sort_values('Absolute Value Correlation', ascending=False, inplace=True)\n",
    "\n",
    "# #print output\n",
    "# correlation_summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ### CALCULATE CORRELATION OF EACH POSSIBLE INPUT vs. REVENUE VALUE ---ONEHOT CATEGORICAL DF---  TO HELP DECISION MAKING OF WHICH INPUTS TO INCLUDE###\n",
    "# categ_correlation_summary_list = []\n",
    "\n",
    "# for col in df_categorical_onehot.columns:\n",
    "#     #onehotencoded categorical columns are all 0/1 ints, so can calculate all correlations\n",
    "        \n",
    "#     #having NANs in the dataset for correlations breaks the correlation calculation\n",
    "#     #so only keep the good_rows that don't have nans for either series being used in the correlation calculation\n",
    "#     #and then np.compress(good_rows, series) just reduces the series to the array with only the good_rows to run the correlation\n",
    "#     good_rows = ~np.logical_or(np.isnan(df_categorical_onehot[col]), np.isnan(df.totals_transactionRevenue))\n",
    "\n",
    "#     #pearsonr function calculates the Pearson correlation coefficient and the p-value for as a tuple\n",
    "#     correl_pvalue = pearsonr(np.compress(good_rows, df_categorical_onehot[col]), np.compress(good_rows, df.totals_transactionRevenue))\n",
    "\n",
    "#     #create new tuple that also has the column name (which is the input variable) and the correl coef and p-value\n",
    "#     variable_correl_pvalue = (col,) + correl_pvalue\n",
    "\n",
    "#     #add the correl and pvalue tuple to the list of all correlation summaries\n",
    "#     categ_correlation_summary_list.append(variable_correl_pvalue)\n",
    "\n",
    "        \n",
    "# #create a dataframe of the correlation summary (for ease of readibility/manipulation)    \n",
    "# categ_correlation_summary_df = pd.DataFrame(categ_correlation_summary_list, columns=['Input Variable', 'Correlation', 'p-value'])\n",
    "# #create a new column of absolute value of correlations so can easily sort both positive and negative correlations together\n",
    "# categ_correlation_summary_df['Absolute Value Correlation'] = categ_correlation_summary_df.Correlation.map(lambda correl: abs(correl))\n",
    "# #sort the dataframe by absolute value correlation high to low\n",
    "# categ_correlation_summary_df.sort_values('Absolute Value Correlation', ascending=False, inplace=True)\n",
    "\n",
    "# #print output\n",
    "# categ_correlation_summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of all of our variables being used for the model (before dropping nans):  (903652, 14)\n",
      "\n",
      "Shape of all of our variables being used for the model (after dropping nans):  (902077, 14)\n",
      "\n",
      "Shape of X input variables is:  (902077, 13) \n",
      "Shape of y output variable is:  (902077,)\n"
     ]
    }
   ],
   "source": [
    "### ASSIGN X and y DATA for VARIALBES WE WANT TO USE###\n",
    "\n",
    "# for X data use the initial correlation values and variables that we think are most important to narrow things down\n",
    "# (remember, that the correlation values are just linear correlation values, so this doesn't capture variables\n",
    "# that do have a large influence but might be nonlinear, however, for linear regression models at the least, that\n",
    "# seems like a good metric to start with as the linear models won't be able to capture nonlinear affects well anyways)\n",
    "\n",
    "#INITIAL RUN DECISIONS: as we can see from the initial Pearson correlations, no variables even fall within the range of what\n",
    "#we would consider even low correlations traditionally, so it is doubtful that linear regression models will work well,\n",
    "#but we'll try it out - take roughly top 10 variables, use all one hot encoded columns and also include\n",
    "#weekday_local, month_local, yearday_local, and hour_local since those are features we specifically added to\n",
    "#make our features unique\n",
    "\n",
    "### NARROW DOWN THE CATEGORICAL COLUMNS WANT TO ADD AS X VARIABLE INPUTS ###\n",
    "categorical_columns_x_model = ['trafficSource_isTrueDirect', 'trafficSource_source',\n",
    "                               #'trafficSource_keyword', #tons of dimensions and not good predictor\n",
    "                                'geoNetwork_continent', 'geoNetwork_country'\n",
    "                               #'geoNetwork_city'\n",
    "                               ]\n",
    "\n",
    "\n",
    "### NARROW DOWN THE NUMERICAL COLUMNS WANT TO ADD TO X VARIABLE INPUTS ###\n",
    "numerical_columns_x_model = ['totals_pageviews', 'totals_hits', 'visitNumber', 'totals_newVisits', 'totals_bounces',\n",
    "                             'weekday_local', 'month_local', 'yearday_local', 'hour_local']\n",
    "\n",
    "#create y outputs column name (but do in list form for easy list adding later)\n",
    "column_y_model = ['totals_transactionRevenue']\n",
    "\n",
    "#create the model dataframe that includes chosen x input variables (from numerical and categorical) and y output variable\n",
    "#do this so that can clean the dataframe by dropping all rows that have any nans\n",
    "df_model = df[numerical_columns_x_model + categorical_columns_x_model + column_y_model]\n",
    "\n",
    "print('\\nShape of all of our variables being used for the model (before dropping nans): ', df_model.shape)\n",
    "\n",
    "#for linear regression drop NANs as they can't be interpreted in the regression model - check to make\n",
    "#sure it isn't reducing size of data too much before proceeding\n",
    "df_model = df_model.dropna(axis='index', how='any')\n",
    "print('\\nShape of all of our variables being used for the model (after dropping nans): ', df_model.shape)\n",
    "\n",
    "#add a column to the df_model data of a simple classifier of \"revenue\" or \"no_revenue\" - will use this data point for:\n",
    "#     in the train_test_split model we will use the stratify command to get equal train-test percentages for both revenue\n",
    "#     and no revenue outcomes - I think this will be important since only about 1.3% of all rows actually resulted in \n",
    "#     revenue and not completely sure how randomly selecting will have equal test-train distributions without defining it\n",
    "#     (this may be unnecessary, but better safe than sorry)\n",
    "df_model['revenue_label'] = df_model.totals_transactionRevenue.map(lambda revenue_amount: \n",
    "                                                        'revenue' if revenue_amount > 0 else 'no_revenue')\n",
    "\n",
    "\n",
    "#split out the data we are using for modeling to X and y values\n",
    "columns_X_model = [col for col in list(df_model.columns) if col not in ['totals_transactionRevenue', 'revenue_label']]\n",
    "X_model = df_model[columns_X_model]\n",
    "\n",
    "#don't actually need to reshape the y_model data for decision trees apparently, but narrow it down to only y_values\n",
    "y_model = df_model['totals_transactionRevenue'] #.values.reshape(-1, 1)\n",
    "\n",
    "#put stratify criteria of revenue/no_revenue into its own array, make sure to reshape this as well\n",
    "stratify_criteria_model = df_model['revenue_label'] #.values.reshape(-1, 1)\n",
    "\n",
    "print('\\nShape of X input variables is: ', X_model.shape, '\\nShape of y output variable is: ', y_model.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check Shapes of the train-test data splits.\n",
      "\n",
      "X_model_train:  (676557, 13)\n",
      "X_model_test:  (225520, 13)\n",
      "y_model_train:  (676557,)\n",
      "y_model_test:  (225520,)\n",
      "\n",
      "--------------------------------------------------------------------\n",
      "Check that the train-test data split worked along stratify criteria.\n",
      "\n",
      "The df_model data percentages of revenue and no_revenue are:\n",
      "no_revenue    0.987242\n",
      "revenue       0.012758\n",
      "Name: revenue_label, dtype: float64\n",
      "\n",
      "The percentage of model_train data that has revenue is:  0.01275871803853925\n",
      "\n",
      "The percentage of model_test data that has revenue is:  0.012757183398368215\n"
     ]
    }
   ],
   "source": [
    "##### TRAIN-TEST-SPLIT #####\n",
    "\n",
    "### SPLIT THE MODEL DATA ###\n",
    "#split the model data (which is all of the Kaggle Training data) into the model's train/test subsets\n",
    "#(have to do this since Kaggle competition has its own test data, but those actual values are not provided, so can't\n",
    "#actually use that to test our models, just end up comparing our predictions on that test data with their actuals)\n",
    "#use a 75-25 split to start with train-test\n",
    "#also make sure to add stratify_criteria to make sure it is doing a 75-25 split on both website visits that led to \n",
    "#actual sales/revenue and those that did not\n",
    "X_model_train, X_model_test, y_model_train, y_model_test = train_test_split(X_model, y_model,\n",
    "                                                                            test_size=0.25,\n",
    "                                                                            stratify=stratify_criteria_model)\n",
    "#print sizes of the train/test data splits\n",
    "print('Check Shapes of the train-test data splits.\\n')\n",
    "print('X_model_train: ', X_model_train.shape)\n",
    "print('X_model_test: ', X_model_test.shape)\n",
    "print('y_model_train: ', y_model_train.shape)\n",
    "print('y_model_test: ', y_model_test.shape)\n",
    "\n",
    "\n",
    "### VERIFTY THE STRATEFIY COMMAND WORKED ###\n",
    "print('\\n--------------------------------------------------------------------')\n",
    "print('Check that the train-test data split worked along stratify criteria.\\n')\n",
    "\n",
    "# FIRST PRINT OUT TOTAL DF_MODEL PERCENTAGE OF DATA THAT HAS REVENUE #\n",
    "print('The df_model data percentages of revenue and no_revenue are:')\n",
    "#since this data from the df_model is in a series, we can just use pandas value counts\n",
    "print(df_model['revenue_label'].value_counts(normalize=True))\n",
    "\n",
    "# THEN CHECK THE TRAIN DATA #\n",
    "#the train data is now an array, so can't use value_counts, have to use np commands\n",
    "#filter the data to be only y_values that had revenue (rev>0) by using the np.where command\n",
    "#it creates a mask of booleans that you can use to filter your array based on whatever criteria you give it\n",
    "#take the length of this filtered array to figure out how many y_values actually had revenue\n",
    "y_model_train_revenue_count = len(y_model_train.values.reshape(-1, 1)[np.where(y_model_train.values.reshape(-1, 1) > 0)])\n",
    "#calculate the percentage of the y_values that have revenue by taking the revenue count/total count in that dataset \n",
    "#note: this percentage should equal the overall percentage of your data that has revenue if the stratify command is working properly\n",
    "y_model_train_revenue_percent = y_model_train_revenue_count/y_model_train.shape[0]\n",
    "print('\\nThe percentage of model_train data that has revenue is: ', y_model_train_revenue_percent)\n",
    "\n",
    "# THEN CHECK THE TEST DATA #\n",
    "#the test data is now an array, so can't use value_counts, have to use np commands\n",
    "#filter the data to be only y_values that had revenue (rev>0) by using the np.where command\n",
    "#it creates a mask of booleans that you can use to filter your array based on whatever criteria you give it\n",
    "#take the length of this filtered array to figure out how many y_values actually had revenue\n",
    "y_model_test_revenue_count = len(y_model_test.values.reshape(-1, 1)[np.where(y_model_test.values.reshape(-1, 1) > 0)])\n",
    "#calculate the percentage of the y_values that have revenue by taking the revenue count/total count in that dataset \n",
    "#note: this percentage should equal the overall percentage of your data that has revenue if the stratify command is working properly\n",
    "y_model_test_revenue_percent = y_model_test_revenue_count/y_model_test.shape[0]\n",
    "print('\\nThe percentage of model_test data that has revenue is: ', y_model_test_revenue_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest v3\n",
    "### Version 3: Limited Input Variables, Label Encoding, No Scaling/Transforming of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initial Functions Used for Evaluation of Models\n",
    "These functions are being created to evaluate whether models are even identifying transactions correctly as revenue/no revenue (or even worse if it assigned negative revenue) because beyond the final revenue amount we want predicted, we also don't want predictions of revenue (or negative revenue) for someone that didn't buy anything and had 0 revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to define whether the outcome is revenue (when revenue_amt is greater than 0), \n",
    "#no_revenue (when revenue_amt is 0), and neg_revenue (when revenue_amt is less than 0)\n",
    "def revenue_norevenue_negrevenue(revenue_amt):\n",
    "    if revenue_amt > 0:\n",
    "        return 'revenue'\n",
    "    elif revenue_amt == 0:\n",
    "        return 'no_revenue'\n",
    "    elif revenue_amt < 0:\n",
    "        return 'neg_revenue'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### CREATE A FUNCTION THAT EVALUATES THE REVENUE OR NO_REVENUE ACCURACY ###\n",
    "#an important part of this model is to make sure that only people that actually performed a final transaction are \n",
    "#getting a revenue prediction\n",
    "#so calculate the percentages of each outcome in a confusion matrix using sklearn.metrics.confusion_matrix:\n",
    "# --True Positive: Revenue_actual & Revenue_predicted\n",
    "# --True Negative: No_Revenue_actual & No_Revenue_predicted\n",
    "# --False Negative: Revenue_actual & No_Revenue_predicted\n",
    "# --False Positive: No_Revenue_actual & Revenue_predicted\n",
    "\n",
    "#inputs of y_revenue_true, y_revenue_predicted are the actual revenue amount arrays,\n",
    "#we will convert to labels\n",
    "def evaluate_revenue_confusion_matrix(y_revenue_true, y_revenue_predicted):\n",
    "    df_revenue_eval = pd.DataFrame(data={'revenue_actual': y_revenue_true, #y_revenue_true.reshape(-1),\n",
    "                       'revenue_prediction': y_revenue_predicted}) #y_revenue_predicted.reshape(-1)})\n",
    "    \n",
    "    df_revenue_eval['revenue_label_actual'] = df_revenue_eval.revenue_actual.map(revenue_norevenue_negrevenue)\n",
    "    \n",
    "    df_revenue_eval['revenue_label_prediction'] = df_revenue_eval.revenue_prediction.map(revenue_norevenue_negrevenue)\n",
    "    \n",
    "    confusion_matrix_revenue = confusion_matrix(df_revenue_eval.revenue_label_actual,\n",
    "                                                 df_revenue_eval.revenue_label_prediction,\n",
    "                                                 labels=['revenue', 'no_revenue', 'neg_revenue'])\n",
    "    \n",
    "    print('Confusion Matrix of revenue/no_revenue/neg_revenue: \\n')\n",
    "    #use scikit learns confusion matrix (the diagonal indicates true hits, outside of that is the false hits)\n",
    "    print(confusion_matrix_revenue)\n",
    "    return confusion_matrix_revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#function that saves the machine learning model to a pickle, so we can extract it later\n",
    "#input of file_name_path, make sure it includes extension of .pkl\n",
    "def save_model_pickle(scikit_model, file_name_path):\n",
    "    filename = file_name_path\n",
    "    pickle.dump(scikit_model, open(filename, 'wb'))\n",
    "    print(\"Scikit Model saved to {}\".format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressor: n_estimators=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.11126265  0.17725263  0.33581702  0.00115248  0.          0.05170576\n",
      "  0.02572765  0.10376375  0.12643741  0.03596573  0.0210055   0.00422666\n",
      "  0.00568275]\n"
     ]
    }
   ],
   "source": [
    "regr = RandomForestRegressor(#max_depth=10, \n",
    "                             #random_state=0,\n",
    "                              n_estimators=5)\n",
    "regr.fit(X_model_train, y_model_train)\n",
    "print(regr.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('totals_pageviews', 0.11126264862179994),\n",
       " ('totals_hits', 0.17725263162549143),\n",
       " ('visitNumber', 0.3358170209373692),\n",
       " ('totals_newVisits', 0.0011524846169580572),\n",
       " ('totals_bounces', 0.0),\n",
       " ('weekday_local', 0.051705760429027414),\n",
       " ('month_local', 0.025727653059584715),\n",
       " ('yearday_local', 0.10376375174869798),\n",
       " ('hour_local', 0.12643741155470672),\n",
       " ('trafficSource_isTrueDirect', 0.035965725664915599),\n",
       " ('trafficSource_source', 0.021005496979401708),\n",
       " ('geoNetwork_continent', 0.00422666255150108),\n",
       " ('geoNetwork_country', 0.0056827522105460607)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show the X columns input and their associated feature importance\n",
    "list(zip(X_model_train.columns, regr.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0. ...,  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "y_model_test_predict = regr.predict(X_model_test)\n",
    "print(y_model_test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix of revenue/no_revenue/neg_revenue: \n",
      "\n",
      "[[  2212    665      0]\n",
      " [  6620 216023      0]\n",
      " [     0      0      0]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_revenue_confusion_matrix(y_model_test, y_model_test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9676968783256474\n"
     ]
    }
   ],
   "source": [
    "percent_accuracy = (2212 + 216023)/(2212 + 216023 + 665 + 6620)\n",
    "print(percent_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 2920533384289471.0 \n",
      "R-squared: -0.6260142518112775 \n",
      "Correlation: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael_suomi/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:9: RuntimeWarning: invalid value encountered in sqrt\n"
     ]
    }
   ],
   "source": [
    "### EVALUATE THE MODEL USING MSE, R2, CORREL ###\n",
    "\n",
    "#MSE function syntax: mean_squared_error(y_true, y_pred, sample_weight=None, multioutput=’uniform_average’)\n",
    "MSE = mean_squared_error(y_model_test, y_model_test_predict)\n",
    "\n",
    "#R2 function syntax: r2_score(y_true, y_pred, sample_weight=None, multioutput=’uniform_average’)\n",
    "r2 = r2_score(y_model_test, y_model_test_predict)\n",
    "\n",
    "print(\"Mean Squared Error: {} \\nR-squared: {} \\nCorrelation: {}\".format(MSE, r2, np.sqrt(r2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit Model saved to model_pickles/random_forest_v3_n_estimators_5.pkl\n"
     ]
    }
   ],
   "source": [
    "save_model_pickle(regr, 'model_pickles/random_forest_v3_n_estimators_5.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressor: n_estimators=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.19650739e-01   1.58621411e-01   3.48609827e-01   3.33071746e-03\n",
      "   7.43124379e-05   4.50196060e-02   3.63449531e-02   1.17564016e-01\n",
      "   1.07174648e-01   2.38906109e-02   2.86787169e-02   3.54385603e-03\n",
      "   7.49658557e-03]\n"
     ]
    }
   ],
   "source": [
    "regr = RandomForestRegressor(#max_depth=10, \n",
    "                             #random_state=0,\n",
    "                              n_estimators=50)\n",
    "regr.fit(X_model_train, y_model_train)\n",
    "print(regr.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('totals_pageviews', 0.11965073917833095),\n",
       " ('totals_hits', 0.15862141131174681),\n",
       " ('visitNumber', 0.34860982727104151),\n",
       " ('totals_newVisits', 0.0033307174551724067),\n",
       " ('totals_bounces', 7.4312437928260579e-05),\n",
       " ('weekday_local', 0.045019606012241517),\n",
       " ('month_local', 0.036344953094142464),\n",
       " ('yearday_local', 0.11756401614587261),\n",
       " ('hour_local', 0.10717464772263961),\n",
       " ('trafficSource_isTrueDirect', 0.023890610904593199),\n",
       " ('trafficSource_source', 0.028678716857540181),\n",
       " ('geoNetwork_continent', 0.0035438560340496756),\n",
       " ('geoNetwork_country', 0.0074965855747007515)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show the X columns input and their associated feature importance\n",
    "list(zip(X_model_train.columns, regr.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0. ...,  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "y_model_test_predict = regr.predict(X_model_test)\n",
    "print(y_model_test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix of revenue/no_revenue/neg_revenue: \n",
      "\n",
      "[[  2747    130      0]\n",
      " [ 13098 209545      0]\n",
      " [     0      0      0]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_revenue_confusion_matrix(y_model_test, y_model_test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9413444483859524\n"
     ]
    }
   ],
   "source": [
    "percent_accuracy = (2747 + 209545)/(2747 + 209545 + 130 + 13098)\n",
    "print(percent_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 2360893613788940.0 \n",
      "R-squared: -0.314433412636709 \n",
      "Correlation: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael_suomi/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:9: RuntimeWarning: invalid value encountered in sqrt\n"
     ]
    }
   ],
   "source": [
    "### EVALUATE THE MODEL USING MSE, R2, CORREL ###\n",
    "\n",
    "#MSE function syntax: mean_squared_error(y_true, y_pred, sample_weight=None, multioutput=’uniform_average’)\n",
    "MSE = mean_squared_error(y_model_test, y_model_test_predict)\n",
    "\n",
    "#R2 function syntax: r2_score(y_true, y_pred, sample_weight=None, multioutput=’uniform_average’)\n",
    "r2 = r2_score(y_model_test, y_model_test_predict)\n",
    "\n",
    "print(\"Mean Squared Error: {} \\nR-squared: {} \\nCorrelation: {}\".format(MSE, r2, np.sqrt(r2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit Model saved to model_pickles/random_forest_v3_n_estimators_50.pkl\n"
     ]
    }
   ],
   "source": [
    "save_model_pickle(regr, 'model_pickles/random_forest_v3_n_estimators_50.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressor: n_estimators=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.61259092e-01   1.64862619e-01   2.86082071e-01   3.96530490e-03\n",
      "   2.42040426e-05   4.66017631e-02   2.98535442e-02   1.34829436e-01\n",
      "   1.19706893e-01   1.03818876e-02   2.87186442e-02   3.05347214e-03\n",
      "   1.06610691e-02]\n"
     ]
    }
   ],
   "source": [
    "regr = RandomForestRegressor(#max_depth=10, \n",
    "                             #random_state=0,\n",
    "                              n_estimators=200)\n",
    "regr.fit(X_model_train, y_model_train)\n",
    "print(regr.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('totals_pageviews', 0.16125909214228515),\n",
       " ('totals_hits', 0.16486261933041405),\n",
       " ('visitNumber', 0.28608207063141722),\n",
       " ('totals_newVisits', 0.003965304899219084),\n",
       " ('totals_bounces', 2.4204042594456747e-05),\n",
       " ('weekday_local', 0.046601763142988216),\n",
       " ('month_local', 0.02985354417914108),\n",
       " ('yearday_local', 0.13482943595968278),\n",
       " ('hour_local', 0.11970689257243412),\n",
       " ('trafficSource_isTrueDirect', 0.010381887608111338),\n",
       " ('trafficSource_source', 0.028718644247797687),\n",
       " ('geoNetwork_continent', 0.0030534721430498083),\n",
       " ('geoNetwork_country', 0.010661069100865249)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show the X columns input and their associated feature importance\n",
    "list(zip(X_model_train.columns, regr.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0. ...,  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "y_model_test_predict = regr.predict(X_model_test)\n",
    "print(y_model_test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix of revenue/no_revenue/neg_revenue: \n",
      "\n",
      "[[  2824     53      0]\n",
      " [ 16084 206559      0]\n",
      " [     0      0      0]]\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix_200 = evaluate_revenue_confusion_matrix(y_model_test, y_model_test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.928445370699\n"
     ]
    }
   ],
   "source": [
    "### ACCURACY OF REVENUE/NO_REVENUE CATEGORY LABELS ###\n",
    "\n",
    "#use np.sum(array) to sum all the values in the array\n",
    "percent_accuracy = (confusion_matrix_200[0][0] + confusion_matrix_200[1][1])/np.sum(confusion_matrix_200)\n",
    "print(percent_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 3292175127422544.5 \n",
      "R-squared: 0.007933613632932435 \n",
      "Correlation: 0.08907083491767906\n"
     ]
    }
   ],
   "source": [
    "### EVALUATE THE MODEL USING MSE, R2, CORREL ###\n",
    "\n",
    "#MSE function syntax: mean_squared_error(y_true, y_pred, sample_weight=None, multioutput=’uniform_average’)\n",
    "MSE = mean_squared_error(y_model_test, y_model_test_predict)\n",
    "\n",
    "#R2 function syntax: r2_score(y_true, y_pred, sample_weight=None, multioutput=’uniform_average’)\n",
    "r2 = r2_score(y_model_test, y_model_test_predict)\n",
    "\n",
    "print(\"Mean Squared Error: {} \\nR-squared: {} \\nCorrelation: {}\".format(MSE, r2, np.sqrt(r2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=None,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit Model saved to model_pickles/random_forest_v3_n_estimators_200.pkl\n"
     ]
    }
   ],
   "source": [
    "save_model_pickle(regr, 'model_pickles/random_forest_v3_n_estimators_200.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Depths of All of Our Random Forests: \n",
      "Min Depth:  29\n",
      "Mean Depth:  32.88\n",
      "Max Depth:  38\n",
      "\n",
      "Decision Tree Depths Distribution: \n",
      "25th/50th/75th Percentiles:  [ 32.  33.  34.]\n"
     ]
    }
   ],
   "source": [
    "### LOOK AT THE DEPTH OF ALL OF OUR DECISION TREES USED IN THE RANDOM FOREST TREES ###\n",
    "#and evaluate the average, min, max and some distributions\n",
    "decision_tree_depths= [estimator.tree_.max_depth for estimator in regr.estimators_] \n",
    "\n",
    "print('Decision Tree Depths of All of Our Random Forests: ')\n",
    "print('Min Depth: ', min(decision_tree_depths))\n",
    "print('Mean Depth: ', np.mean(decision_tree_depths))\n",
    "print('Max Depth: ', max(decision_tree_depths))\n",
    "\n",
    "print('\\nDecision Tree Depths Distribution: ')\n",
    "print('25th/50th/75th Percentiles: ', np.percentile(decision_tree_depths, q=[25, 50, 75]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The example decision tree we are looking at has a maximum depth of:  33\n"
     ]
    }
   ],
   "source": [
    "### VISUALIZE ONE OF THE DECISION TREES IN OUR RANDOM FOREST USING GRAPHVIZ - TRY TO PICK ONE WITH AVERAGE DEPTH OF OUR RF ###\n",
    "#find an individual decision tree (estimator) that has the median depth and try to visualize it to see the\n",
    "#end leaf outputs, etc. to better understand the model\n",
    "\n",
    "#make sure have already imported the following packages (and make sure to install graphivz)\n",
    "# from sklearn.tree import export_graphviz\n",
    "# from subprocess import call\n",
    "# from IPython.display import Image\n",
    "\n",
    "#manually try different estimators_ in list until verify that max_depth does match closely to the mean/median depth\n",
    "decision_tree_avg_depth_example = regr.estimators_[0]\n",
    "print('The example decision tree we are looking at has a maximum depth of: ', decision_tree_avg_depth_example.tree_.max_depth)\n",
    "\n",
    "#export our full tree as a dot file (which is the filetype that graphviz uses)\n",
    "export_graphviz(decision_tree_avg_depth_example, #the tree regressor\n",
    "                out_file='model_pickles/avg_tree_viz_random_forest_v3_n_estimators_200.dot', \n",
    "                feature_names = list(X_model_train.columns),\n",
    "#                 class_names = iris.target_names,\n",
    "                rounded = True, #rounded When set to True, draw node boxes with rounded corners and use Helvetica fonts instead of Times-Roman.\n",
    "                proportion = True, #proportion True changes 'values' and/or 'samples' to be proportions and percentages respectively instead of counts \n",
    "                precision = 2, filled = True,\n",
    "                leaves_parallel = True) #leaves_parallel when set to True, draw all leaf nodes at the bottom of the tree.\n",
    "\n",
    "#export top 5 levels of our tree as a dot file - the full tree representation is massive, so this might help the viz actually run\n",
    "export_graphviz(decision_tree_avg_depth_example, #the tree regressor\n",
    "                out_file='model_pickles/avg_tree_depth5_viz_random_forest_v3_n_estimators_200.dot',\n",
    "                max_depth=5,\n",
    "                feature_names = list(X_model_train.columns),\n",
    "#                 class_names = iris.target_names,\n",
    "                rounded = True, #rounded When set to True, draw node boxes with rounded corners and use Helvetica fonts instead of Times-Roman.\n",
    "                proportion = True, #proportion True changes 'values' and/or 'samples' to be proportions and percentages respectively instead of counts \n",
    "                precision = 2, filled = True,\n",
    "                leaves_parallel = True) #leaves_parallel when set to True, draw all leaf nodes at the bottom of the tree.\n",
    "\n",
    "# VISUALIZE AT www.webgraphviz.com: can visualize the .dot outputs, by coping the .dot file txt into http://www.webgraphviz.com/\n",
    "\n",
    "# # Convert to png using system command (requires Graphviz -unsure how to install this on cloud server)\n",
    "# # ExecutableNotFound: failed to execute ['dot', '-Tsvg'], make sure the Graphviz executables are on your systems' PATH\n",
    "# #option1- try to display top 5 levels with graphviz - \n",
    "#Source.from_file('model_pickles/avg_tree_depth5_viz_random_forest_v3_n_estimators_200.dot')\n",
    "\n",
    "# #option2- Command line syntax call is trying to replicate: !dot -Tpng tree.dot -o tree.png -Gdpi=600\n",
    "# call(['dot', '-Tpng', 'model_pickles/avg_tree_viz_random_forest_v3_n_estimators_200.dot', \n",
    "#       '-o', 'model_pickles/avg_tree_viz_random_forest_v3_n_estimators_200.png', '-Gdpi=600'])\n",
    "\n",
    "# #Display in jupyter notebook\n",
    "# Image(filename = 'model_pickles/avg_tree_viz_random_forest_v3_n_estimators_200.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressor: n_estimators=200, min_samples_leaf=44\n",
    "Chose min_samples_leaf of 44, which is approximately 0.005 (a half a percent) of the count of revenue transactions in the training dataset.  (approx 900,000 rows x .75 for training set x .013 conversion rate x .005 half a percent chosen arbitrarily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  3.54060163e-01   5.97716218e-02   2.95349613e-01   8.72878401e-03\n",
      "   2.36497981e-09   2.02961966e-02   4.81975359e-03   9.62143878e-02\n",
      "   3.86690256e-02   6.42305665e-03   8.40104265e-02   7.03941642e-03\n",
      "   2.46175523e-02]\n"
     ]
    }
   ],
   "source": [
    "regr = RandomForestRegressor(#max_depth=10, \n",
    "                             #random_state=0,\n",
    "                             min_samples_leaf=44,\n",
    "                             n_estimators=200)\n",
    "regr.fit(X_model_train, y_model_train)\n",
    "print(regr.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('totals_pageviews', 0.35406016333873763),\n",
       " ('totals_hits', 0.059771621774540824),\n",
       " ('visitNumber', 0.29534961294653461),\n",
       " ('totals_newVisits', 0.0087287840056264072),\n",
       " ('totals_bounces', 2.3649798142344095e-09),\n",
       " ('weekday_local', 0.020296196643200721),\n",
       " ('month_local', 0.0048197535892391058),\n",
       " ('yearday_local', 0.096214387827991021),\n",
       " ('hour_local', 0.038669025643228626),\n",
       " ('trafficSource_isTrueDirect', 0.0064230566456778861),\n",
       " ('trafficSource_source', 0.084010426485637654),\n",
       " ('geoNetwork_continent', 0.0070394164231517287),\n",
       " ('geoNetwork_country', 0.024617552311453895)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show the X columns input and their associated feature importance\n",
    "list(zip(X_model_train.columns, regr.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0. ...,  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "y_model_test_predict = regr.predict(X_model_test)\n",
    "print(y_model_test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix of revenue/no_revenue/neg_revenue: \n",
      "\n",
      "[[  2870      7      0]\n",
      " [ 33858 188785      0]\n",
      " [     0      0      0]]\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix_model = evaluate_revenue_confusion_matrix(y_model_test, y_model_test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.849835934729\n"
     ]
    }
   ],
   "source": [
    "### ACCURACY OF REVENUE/NO_REVENUE CATEGORY LABELS ###\n",
    "\n",
    "#use np.sum(array) to sum all the values in the array\n",
    "percent_accuracy = (confusion_matrix_model[0][0] + confusion_matrix_model[1][1])/np.sum(confusion_matrix_model)\n",
    "print(percent_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 3115496866485701.5 \n",
      "R-squared: 0.06117396601189473 \n",
      "Correlation: 0.24733371386023123\n"
     ]
    }
   ],
   "source": [
    "### EVALUATE THE MODEL USING MSE, R2, CORREL ###\n",
    "\n",
    "#MSE function syntax: mean_squared_error(y_true, y_pred, sample_weight=None, multioutput=’uniform_average’)\n",
    "MSE = mean_squared_error(y_model_test, y_model_test_predict)\n",
    "\n",
    "#R2 function syntax: r2_score(y_true, y_pred, sample_weight=None, multioutput=’uniform_average’)\n",
    "r2 = r2_score(y_model_test, y_model_test_predict)\n",
    "\n",
    "print(\"Mean Squared Error: {} \\nR-squared: {} \\nCorrelation: {}\".format(MSE, r2, np.sqrt(r2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=44, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=None,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit Model saved to model_pickles/random_forest_v3_n_estimators_200_min_leaf_44.pkl\n"
     ]
    }
   ],
   "source": [
    "save_model_pickle(regr, 'model_pickles/random_forest_v3_n_estimators_200_min_leaf_44.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Depths of All of Our Random Forests: \n",
      "Min Depth:  16\n",
      "Mean Depth:  18.995\n",
      "Max Depth:  25\n",
      "\n",
      "Decision Tree Depths Distribution: \n",
      "25th/50th/75th Percentiles:  [ 18.  19.  20.]\n"
     ]
    }
   ],
   "source": [
    "### LOOK AT THE DEPTH OF ALL OF OUR DECISION TREES USED IN THE RANDOM FOREST TREES ###\n",
    "#and evaluate the average, min, max and some distributions\n",
    "decision_tree_depths= [estimator.tree_.max_depth for estimator in regr.estimators_] \n",
    "\n",
    "print('Decision Tree Depths of All of Our Random Forests: ')\n",
    "print('Min Depth: ', min(decision_tree_depths))\n",
    "print('Mean Depth: ', np.mean(decision_tree_depths))\n",
    "print('Max Depth: ', max(decision_tree_depths))\n",
    "\n",
    "print('\\nDecision Tree Depths Distribution: ')\n",
    "print('25th/50th/75th Percentiles: ', np.percentile(decision_tree_depths, q=[25, 50, 75]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The example decision tree we are looking at has a maximum depth of:  19\n"
     ]
    }
   ],
   "source": [
    "### VISUALIZE ONE OF THE DECISION TREES IN OUR RANDOM FOREST USING GRAPHVIZ - TRY TO PICK ONE WITH AVERAGE DEPTH OF OUR RF ###\n",
    "#find an individual decision tree (estimator) that has the median depth and try to visualize it to see the\n",
    "#end leaf outputs, etc. to better understand the model\n",
    "\n",
    "#make sure have already imported the following packages (and make sure to install graphivz)\n",
    "# from sklearn.tree import export_graphviz\n",
    "# from subprocess import call\n",
    "# from IPython.display import Image\n",
    "\n",
    "#manually try different estimators_ in list until verify that max_depth does match closely to the mean/median depth\n",
    "decision_tree_avg_depth_example = regr.estimators_[0]\n",
    "print('The example decision tree we are looking at has a maximum depth of: ', decision_tree_avg_depth_example.tree_.max_depth)\n",
    "\n",
    "#export our full tree as a dot file (which is the filetype that graphviz uses)\n",
    "export_graphviz(decision_tree_avg_depth_example, #the tree regressor\n",
    "                out_file='model_pickles/avg_tree_viz_random_forest_v3_n_estimators_200_min_leaf_44.dot', \n",
    "                feature_names = list(X_model_train.columns),\n",
    "#                 class_names = iris.target_names,\n",
    "                rounded = True, #rounded When set to True, draw node boxes with rounded corners and use Helvetica fonts instead of Times-Roman.\n",
    "                proportion = True, #proportion True changes 'values' and/or 'samples' to be proportions and percentages respectively instead of counts \n",
    "                precision = 2, filled = True,\n",
    "                leaves_parallel = True) #leaves_parallel when set to True, draw all leaf nodes at the bottom of the tree.\n",
    "\n",
    "#export top 5 levels of our tree as a dot file - the full tree representation is massive, so this might help the viz actually run\n",
    "export_graphviz(decision_tree_avg_depth_example, #the tree regressor\n",
    "                out_file='model_pickles/avg_tree_depth5_viz_random_forest_v3_n_estimators_200_min_leaf_44.dot',\n",
    "                max_depth=5,\n",
    "                feature_names = list(X_model_train.columns),\n",
    "#                 class_names = iris.target_names,\n",
    "                rounded = True, #rounded When set to True, draw node boxes with rounded corners and use Helvetica fonts instead of Times-Roman.\n",
    "                proportion = True, #proportion True changes 'values' and/or 'samples' to be proportions and percentages respectively instead of counts \n",
    "                precision = 2, filled = True,\n",
    "                leaves_parallel = True) #leaves_parallel when set to True, draw all leaf nodes at the bottom of the tree.\n",
    "\n",
    "# VISUALIZE AT www.webgraphviz.com: can visualize the .dot outputs, by coping the .dot file txt into http://www.webgraphviz.com/\n",
    "\n",
    "# # Convert to png using system command (requires Graphviz -unsure how to install this on cloud server)\n",
    "# # ExecutableNotFound: failed to execute ['dot', '-Tsvg'], make sure the Graphviz executables are on your systems' PATH\n",
    "# #option1- try to display top 5 levels with graphviz - \n",
    "#Source.from_file('model_pickles/avg_tree_depth5_viz_random_forest_v3_n_estimators_200.dot')\n",
    "\n",
    "# #option2- Command line syntax call is trying to replicate: !dot -Tpng tree.dot -o tree.png -Gdpi=600\n",
    "# call(['dot', '-Tpng', 'model_pickles/avg_tree_viz_random_forest_v3_n_estimators_200.dot', \n",
    "#       '-o', 'model_pickles/avg_tree_viz_random_forest_v3_n_estimators_200.png', '-Gdpi=600'])\n",
    "\n",
    "# #Display in jupyter notebook\n",
    "# Image(filename = 'model_pickles/avg_tree_viz_random_forest_v3_n_estimators_200.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressor: n_estimators=200, min_samples_leaf=8\n",
    "Chose min_samples_leaf of 8, which is approximately 0.001 (a tenth of a percent) of the count of revenue transactions in the training dataset.  (approx 900,000 rows x .75 for training set x .013 conversion rate x .001 one tenth a percent chosen arbitrarily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2.02232248e-01   1.27693002e-01   3.57707897e-01   4.42460963e-03\n",
      "   3.70640498e-08   3.13459637e-02   1.10049756e-02   1.37704508e-01\n",
      "   5.79136174e-02   4.34378079e-03   4.58781029e-02   3.60381663e-03\n",
      "   1.61474406e-02]\n"
     ]
    }
   ],
   "source": [
    "regr = RandomForestRegressor(#max_depth=10, \n",
    "                             #random_state=0,\n",
    "                             min_samples_leaf=8,\n",
    "                             n_estimators=200)\n",
    "regr.fit(X_model_train, y_model_train)\n",
    "print(regr.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('totals_pageviews', 0.2022322476219876),\n",
       " ('totals_hits', 0.12769300240590042),\n",
       " ('visitNumber', 0.35770789741235048),\n",
       " ('totals_newVisits', 0.0044246096333913753),\n",
       " ('totals_bounces', 3.70640498402804e-08),\n",
       " ('weekday_local', 0.031345963707006548),\n",
       " ('month_local', 0.011004975618442039),\n",
       " ('yearday_local', 0.13770450819147612),\n",
       " ('hour_local', 0.057913617398169928),\n",
       " ('trafficSource_isTrueDirect', 0.0043437807922100634),\n",
       " ('trafficSource_source', 0.045878102914293277),\n",
       " ('geoNetwork_continent', 0.0036038166335466992),\n",
       " ('geoNetwork_country', 0.016147440607175701)]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show the X columns input and their associated feature importance\n",
    "list(zip(X_model_train.columns, regr.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0. ...,  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "y_model_test_predict = regr.predict(X_model_test)\n",
    "print(y_model_test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix of revenue/no_revenue/neg_revenue: \n",
      "\n",
      "[[  2860     17      0]\n",
      " [ 23879 198764      0]\n",
      " [     0      0      0]]\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix_model = evaluate_revenue_confusion_matrix(y_model_test, y_model_test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.894040439872\n"
     ]
    }
   ],
   "source": [
    "### ACCURACY OF REVENUE/NO_REVENUE CATEGORY LABELS ###\n",
    "\n",
    "#use np.sum(array) to sum all the values in the array\n",
    "percent_accuracy = (confusion_matrix_model[0][0] + confusion_matrix_model[1][1])/np.sum(confusion_matrix_model)\n",
    "print(percent_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 3198241327108353.0 \n",
      "R-squared: 0.03623969159919882 \n",
      "Correlation: 0.19036725453501402\n"
     ]
    }
   ],
   "source": [
    "### EVALUATE THE MODEL USING MSE, R2, CORREL ###\n",
    "\n",
    "#MSE function syntax: mean_squared_error(y_true, y_pred, sample_weight=None, multioutput=’uniform_average’)\n",
    "MSE = mean_squared_error(y_model_test, y_model_test_predict)\n",
    "\n",
    "#R2 function syntax: r2_score(y_true, y_pred, sample_weight=None, multioutput=’uniform_average’)\n",
    "r2 = r2_score(y_model_test, y_model_test_predict)\n",
    "\n",
    "print(\"Mean Squared Error: {} \\nR-squared: {} \\nCorrelation: {}\".format(MSE, r2, np.sqrt(r2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=8, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=None,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit Model saved to model_pickles/random_forest_v3_n_estimators_200_min_leaf_8.pkl\n"
     ]
    }
   ],
   "source": [
    "save_model_pickle(regr, 'model_pickles/random_forest_v3_n_estimators_200_min_leaf_8.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Depths of All of Our Random Forests: \n",
      "Min Depth:  22\n",
      "Mean Depth:  24.83\n",
      "Max Depth:  33\n",
      "\n",
      "Decision Tree Depths Distribution: \n",
      "25th/50th/75th Percentiles:  [ 24.  25.  26.]\n"
     ]
    }
   ],
   "source": [
    "### LOOK AT THE DEPTH OF ALL OF OUR DECISION TREES USED IN THE RANDOM FOREST TREES ###\n",
    "#and evaluate the average, min, max and some distributions\n",
    "decision_tree_depths= [estimator.tree_.max_depth for estimator in regr.estimators_] \n",
    "\n",
    "print('Decision Tree Depths of All of Our Random Forests: ')\n",
    "print('Min Depth: ', min(decision_tree_depths))\n",
    "print('Mean Depth: ', np.mean(decision_tree_depths))\n",
    "print('Max Depth: ', max(decision_tree_depths))\n",
    "\n",
    "print('\\nDecision Tree Depths Distribution: ')\n",
    "print('25th/50th/75th Percentiles: ', np.percentile(decision_tree_depths, q=[25, 50, 75]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The example decision tree we are looking at has a maximum depth of:  25\n"
     ]
    }
   ],
   "source": [
    "### VISUALIZE ONE OF THE DECISION TREES IN OUR RANDOM FOREST USING GRAPHVIZ - TRY TO PICK ONE WITH AVERAGE DEPTH OF OUR RF ###\n",
    "#find an individual decision tree (estimator) that has the median depth and try to visualize it to see the\n",
    "#end leaf outputs, etc. to better understand the model\n",
    "\n",
    "#make sure have already imported the following packages (and make sure to install graphivz)\n",
    "# from sklearn.tree import export_graphviz\n",
    "# from subprocess import call\n",
    "# from IPython.display import Image\n",
    "\n",
    "#manually try different estimators_ in list until verify that max_depth does match closely to the mean/median depth\n",
    "decision_tree_avg_depth_example = regr.estimators_[0]\n",
    "print('The example decision tree we are looking at has a maximum depth of: ', decision_tree_avg_depth_example.tree_.max_depth)\n",
    "\n",
    "#export our full tree as a dot file (which is the filetype that graphviz uses)\n",
    "export_graphviz(decision_tree_avg_depth_example, #the tree regressor\n",
    "                out_file='model_pickles/avg_tree_viz_random_forest_v3_n_estimators_200_min_leaf_8.dot', \n",
    "                feature_names = list(X_model_train.columns),\n",
    "#                 class_names = iris.target_names,\n",
    "                rounded = True, #rounded When set to True, draw node boxes with rounded corners and use Helvetica fonts instead of Times-Roman.\n",
    "                proportion = True, #proportion True changes 'values' and/or 'samples' to be proportions and percentages respectively instead of counts \n",
    "                precision = 2, filled = True,\n",
    "                leaves_parallel = True) #leaves_parallel when set to True, draw all leaf nodes at the bottom of the tree.\n",
    "\n",
    "#export top 5 levels of our tree as a dot file - the full tree representation is massive, so this might help the viz actually run\n",
    "export_graphviz(decision_tree_avg_depth_example, #the tree regressor\n",
    "                out_file='model_pickles/avg_tree_depth5_viz_random_forest_v3_n_estimators_200_min_leaf_8.dot',\n",
    "                max_depth=5,\n",
    "                feature_names = list(X_model_train.columns),\n",
    "#                 class_names = iris.target_names,\n",
    "                rounded = True, #rounded When set to True, draw node boxes with rounded corners and use Helvetica fonts instead of Times-Roman.\n",
    "                proportion = True, #proportion True changes 'values' and/or 'samples' to be proportions and percentages respectively instead of counts \n",
    "                precision = 2, filled = True,\n",
    "                leaves_parallel = True) #leaves_parallel when set to True, draw all leaf nodes at the bottom of the tree.\n",
    "\n",
    "# VISUALIZE AT www.webgraphviz.com: can visualize the .dot outputs, by coping the .dot file txt into http://www.webgraphviz.com/\n",
    "\n",
    "# # Convert to png using system command (requires Graphviz -unsure how to install this on cloud server)\n",
    "# # ExecutableNotFound: failed to execute ['dot', '-Tsvg'], make sure the Graphviz executables are on your systems' PATH\n",
    "# #option1- try to display top 5 levels with graphviz - \n",
    "#Source.from_file('model_pickles/avg_tree_depth5_viz_random_forest_v3_n_estimators_200.dot')\n",
    "\n",
    "# #option2- Command line syntax call is trying to replicate: !dot -Tpng tree.dot -o tree.png -Gdpi=600\n",
    "# call(['dot', '-Tpng', 'model_pickles/avg_tree_viz_random_forest_v3_n_estimators_200.dot', \n",
    "#       '-o', 'model_pickles/avg_tree_viz_random_forest_v3_n_estimators_200.png', '-Gdpi=600'])\n",
    "\n",
    "# #Display in jupyter notebook\n",
    "# Image(filename = 'model_pickles/avg_tree_viz_random_forest_v3_n_estimators_200.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction of Zero for Everything, What Scores does that Earn?\n",
    "Since 98.7% of the transactions are 0 revenue, would our accuracy and correlations be better if we just used y_predicted as zero for all values?\n",
    "\n",
    "Won't need a model, just need to get length of X_model_test and create y_model_test_predict = all 0s of that length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first list item is:  0 \n",
      "last list item is:  0\n",
      "\n",
      "X_model_test length is:  225520 \n",
      "y_model_test_predict length is:  225520\n"
     ]
    }
   ],
   "source": [
    "#need a list of zeros with length equal to length of X_model_test (which can find that by calling the 0th index of the shape tuple)\n",
    "y_model_test_predict = [0 for row in range(0, X_model_test.shape[0])]\n",
    "\n",
    "#verify that first and last number in list are zero\n",
    "print('first list item is: ', y_model_test_predict[0], '\\nlast list item is: ', y_model_test_predict[-1])\n",
    "\n",
    "#verify lengths\n",
    "print('\\nX_model_test length is: ', X_model_test.shape[0],\n",
    "      '\\ny_model_test_predict length is: ', len(y_model_test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix of revenue/no_revenue/neg_revenue: \n",
      "\n",
      "[[     0   2877      0]\n",
      " [     0 222643      0]\n",
      " [     0      0      0]]\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix_model = evaluate_revenue_confusion_matrix(y_model_test, y_model_test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.987242816602\n"
     ]
    }
   ],
   "source": [
    "### ACCURACY OF REVENUE/NO_REVENUE CATEGORY LABELS ###\n",
    "\n",
    "#use np.sum(array) to sum all the values in the array\n",
    "percent_accuracy = (confusion_matrix_model[0][0] + confusion_matrix_model[1][1])/np.sum(confusion_matrix_model)\n",
    "print(percent_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 3321473856251774.0 \n",
      "R-squared: -0.0008952860791915374 \n",
      "Correlation: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael_suomi/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:9: RuntimeWarning: invalid value encountered in sqrt\n"
     ]
    }
   ],
   "source": [
    "### EVALUATE THE MODEL USING MSE, R2, CORREL ###\n",
    "\n",
    "#MSE function syntax: mean_squared_error(y_true, y_pred, sample_weight=None, multioutput=’uniform_average’)\n",
    "MSE = mean_squared_error(y_model_test, y_model_test_predict)\n",
    "\n",
    "#R2 function syntax: r2_score(y_true, y_pred, sample_weight=None, multioutput=’uniform_average’)\n",
    "r2 = r2_score(y_model_test, y_model_test_predict)\n",
    "\n",
    "print(\"Mean Squared Error: {} \\nR-squared: {} \\nCorrelation: {}\".format(MSE, r2, np.sqrt(r2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

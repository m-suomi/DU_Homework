{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Submission Files Using Our Machine Learning Models\n",
    "\n",
    "## Random Forest Regressor Models - v3\n",
    "### Version 3: Limit Input Variables, Label Encoding, No Scaling/Transforming of Data\n",
    "\n",
    "\n",
    "##### !!!We need to perform the same pre-processing procedure and same variable selection on the Test Dataset as we did the Training Dataset!!! <br> <br>\n",
    "\n",
    "The requirements for this Kaggle competition are defined at the project page:\n",
    "https://www.kaggle.com/c/ga-customer-revenue-prediction\n",
    "\n",
    "#### We need to used our models to make predictions on the test data and then create the final Submission Files in the format required.\n",
    "\n",
    "Root Mean Squared Error (RMSE)\n",
    "Submissions are scored on the root mean squared error. RMSE is defined as:\n",
    "\n",
    "RMSE=1n∑i=1n(yi−y^i)2−−−−−−−−−−−−√,\n",
    "where y hat is the natural log of the predicted revenue for a customer and y is the natural log of the actual summed revenue value plus one.\n",
    "\n",
    "Submission File\n",
    "For each fullVisitorId in the test set, you must predict the natural log of their total revenue in PredictedLogRevenue. The submission file should contain a header and have the following format:\n",
    "\n",
    "fullVisitorId,PredictedLogRevenue <br>\n",
    "0000000259678714014,0 <br>\n",
    "0000049363351866189,0 <br>\n",
    "0000053049821714864,0 <br>\n",
    "etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "\n",
    "# from sklearn import preprocessing\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import mean_squared_error, r2_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and Pre-processing of the Test Dataset\n",
    "We need to perform the same pre-processing procedure and same variable selection on the Test Dataset as we did the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the data engineered and feature engineered test dataset\n",
    "df = pd.read_pickle('/home/michael_suomi/Final-Project-Google-Merch-Store/data/test_v1_full_data_split.pkl')\n",
    "print(df.shape)\n",
    "# print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CHANGE TRANSACTION REVENUE FROM NANs to 0 AND CHANGE to FLOAT TYPE (some are strings)###\n",
    "df.totals_transactionRevenue.fillna(0, inplace=True)\n",
    "df.totals_transactionRevenue = df.totals_transactionRevenue.astype(dtype=float)\n",
    "\n",
    "### CHANGE OTHER STRINGS TO INTS/FLOATS WHERE NEEDED ###\n",
    "#stick to floats rather than ints since a np.nan is a float object\n",
    "df.totals_bounces = df.totals_bounces.astype(dtype=float)\n",
    "df.totals_hits = df.totals_hits.astype(dtype=float)\n",
    "df.totals_newVisits = df.totals_newVisits.astype(dtype=float)\n",
    "df.totals_pageviews = df.totals_pageviews.astype(dtype=float)\n",
    "df.totals_visits = df.totals_visits.astype(dtype=float)\n",
    "\n",
    "### CONVERT NANs in bounces, newVisits to 0 values ###\n",
    "#the blank NAN values for these columns imply a 0 value meaning 0 newVisits or 0 bounces\n",
    "df.totals_bounces.fillna(0, inplace=True)\n",
    "df.totals_newVisits.fillna(0, inplace=True)\n",
    "# df.totals_visits.fillna(0, inplace=True) #there shouldn't be anyone with 0 visits (they've at least visited once or woulnd't be recorded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### VIEW THE DATA BEFORE LABEL ENCODING ###\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view the numerical data columns for counts, mean, and min/max\n",
    "#if the standard deviation (std) is zero, that means every value is the same - may want to check that data\n",
    "#and see if need to edit it (since describe ignores NANs for instance, you may need to go back and convert the NANs to a \n",
    "#value that makes sense)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LABEL ENCODING ALL THE CATEGORICAL VARIABLES ###\n",
    "# label encode the categorical variables\n",
    "categorical_cols = ['channelGrouping', 'socialEngagementType', \n",
    "                   'device_deviceCategory', 'device_browser', 'device_isMobile',\n",
    "                   'device_operatingSystem', 'geoNetwork_subContinent',\n",
    "                   'geoNetwork_region', 'geoNetwork_continent', 'geoNetwork_country',\n",
    "                   'geoNetwork_city', 'geoNetwork_metro', 'geoNetwork_networkDomain',\n",
    "                   'trafficSource_isTrueDirect', 'trafficSource_keyword',\n",
    "                   'trafficSource_source', 'trafficSource_adContent',\n",
    "                   'trafficSource_medium', 'trafficSource_referralPath',\n",
    "                   'trafficSource_campaign']\n",
    "\n",
    "print('Original Dataframe Shape: ', df.shape)\n",
    "\n",
    "for col in categorical_cols:\n",
    "    print('\\n Converting Column: ', col)\n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    lbl.fit(list(df[col].values.astype('str')))\n",
    "    df[col] = lbl.transform(list(df[col].values.astype('str')))\n",
    "    print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decide what Input Data to Use for X and Split Data via train_test_split\n",
    "We need to perform the same pre-processing procedure and same variable selection on the Test Dataset as we did the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ASSIGN X and y DATA for VARIALBES WE WANT TO USE###\n",
    "\n",
    "# for X data use the initial correlation values and variables that we think are most important to narrow things down\n",
    "# (remember, that the correlation values are just linear correlation values, so this doesn't capture variables\n",
    "# that do have a large influence but might be nonlinear, however, for linear regression models at the least, that\n",
    "# seems like a good metric to start with as the linear models won't be able to capture nonlinear affects well anyways)\n",
    "\n",
    "#INITIAL RUN DECISIONS: as we can see from the initial Pearson correlations, no variables even fall within the range of what\n",
    "#we would consider even low correlations traditionally, so it is doubtful that linear regression models will work well,\n",
    "#but we'll try it out - take roughly top 10 variables, use all one hot encoded columns and also include\n",
    "#weekday_local, month_local, yearday_local, and hour_local since those are features we specifically added to\n",
    "#make our features unique\n",
    "\n",
    "### NARROW DOWN THE CATEGORICAL COLUMNS WANT TO ADD AS X VARIABLE INPUTS ###\n",
    "categorical_columns_x_test = ['trafficSource_isTrueDirect', 'trafficSource_source',\n",
    "                               #'trafficSource_keyword', #tons of dimensions and not good predictor\n",
    "                                'geoNetwork_continent', 'geoNetwork_country'\n",
    "                               #'geoNetwork_city'\n",
    "                               ]\n",
    "\n",
    "\n",
    "### NARROW DOWN THE NUMERICAL COLUMNS WANT TO ADD TO X VARIABLE INPUTS ###\n",
    "numerical_columns_x_test = ['totals_pageviews', 'totals_hits', 'visitNumber', 'totals_newVisits', 'totals_bounces',\n",
    "                             'weekday_local', 'month_local', 'yearday_local', 'hour_local']\n",
    "\n",
    "### DON'T HAVE Y-VALUES FOR TEST DATA AS THAT IS WHAT WE ARE PREDICTING ###\n",
    "# #create y outputs column name (but do in list form for easy list adding later)\n",
    "# column_y_model = ['totals_transactionRevenue']\n",
    "\n",
    "#for test data, we now need to keep track of fullVisitorId as well (but do in list form for easy list adding later)\n",
    "column_fullVisitorId_test = ['fullVisitorId']\n",
    "\n",
    "#create the model dataframe that includes chosen x input variables (from numerical and categorical) and y output variable\n",
    "#do this so that can clean the dataframe by dropping all rows that have any nans\n",
    "df_test = df[numerical_columns_x_test + categorical_columns_x_test + column_fullVisitorId_test]\n",
    "\n",
    "### TRY RUNNING MODEL WITHOUT DROPPING NANs FIRST - IF DOESN'T WORK, MAY NEED TO LATER REVISE THIS PROCEDURE ###\n",
    "# print('\\nShape of all of our variables being used for the model (before dropping nans): ', df_test.shape)\n",
    "# #for linear regression drop NANs as they can't be interpreted in the regression model - check to make\n",
    "# #sure it isn't reducing size of data too much before proceeding\n",
    "# df_test = df_test.dropna(axis='index', how='any')\n",
    "# print('\\nShape of all of our variables being used for the model (after dropping nans): ', df_test.shape)\n",
    "\n",
    "### REVENUE_LABEL NOT APPLICABLE FOR TEST DATA AS WE DON'T KNOW ACTUAL REVENUE ###\n",
    "# #add a column to the df_model data of a simple classifier of \"revenue\" or \"no_revenue\" - will use this data point for:\n",
    "# #     in the train_test_split model we will use the stratify command to get equal train-test percentages for both revenue\n",
    "# #     and no revenue outcomes - I think this will be important since only about 1.3% of all rows actually resulted in \n",
    "# #     revenue and not completely sure how randomly selecting will have equal test-train distributions without defining it\n",
    "# #     (this may be unnecessary, but better safe than sorry)\n",
    "# df_model['revenue_label'] = df_model.totals_transactionRevenue.map(lambda revenue_amount: \n",
    "#                                                         'revenue' if revenue_amount > 0 else 'no_revenue')\n",
    "\n",
    "\n",
    "#split out the data we are using for testing to X and y and ID values\n",
    "columns_X_test = [col for col in list(df_model.columns) if col not in ['fullVisitorId', 'totals_transactionRevenue', 'revenue_label']]\n",
    "X_test = df_test[columns_X_test]\n",
    "\n",
    "#split out the fullVisitor info (don't want that column running through model as it doesn't have any coefficients)\n",
    "fullVisitorId_test = df_test['fullVisitorId']\n",
    "\n",
    "### DON'T HAVE Y-VALUES FOR TEST DATA AS THAT IS WHAT WE ARE PREDICTING ###\n",
    "# #don't actually need to reshape the y_model data for decision trees apparently, but narrow it down to only y_values\n",
    "# y_model = df_model['totals_transactionRevenue'] #.values.reshape(-1, 1)\n",
    "\n",
    "### WON'T BE DOING TRAIN-TEST-SPLIT ON THE TEST DATA & DON'T KNOW 'REVENUE_LABEL' SO EXCLUED THIS STEP ###\n",
    "# #put stratify criteria of revenue/no_revenue into its own array, make sure to reshape this as well\n",
    "# stratify_criteria_model = df_model['revenue_label'] #.values.reshape(-1, 1)\n",
    "\n",
    "print('\\nShape of X input variables is: ', X_test.shape, '\\nShape of fullVisitorId is: ', fullVisitorId_test.shape) #, '\\nShape of y output variable is: ', y_model.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Predictions on Test Data with the Archived Trained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initial Functions Used for  Importing Trained Models and Processing Test Predictions to Kaggle Output Required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOAD PICKLE MODEL AND RUN PREDICTION USING THAT MODEL ###\n",
    "\n",
    "#function that loads the machine learning model from a pickle and then uses that to create\n",
    "#the predicted y_values - returns y_predictions\n",
    "#the input of file_name_path, make sure it includes extension of .pkl\n",
    "def prediction_from_pickle_model(X_test_data, model_file_name_path):\n",
    "    loaded_model_regr = pickle.load(open(model_file_name_path, 'rb'))\n",
    "    \n",
    "    y_test_predicted = loaded_model_regr.predict(X_test_data)\n",
    "    \n",
    "    return y_test_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u> Model for Submission:\n",
    "### Random Forest Regressor v3: n_estimators=5\n",
    "model_pickles/random_forest_v3_n_estimators_5.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run predictions using the pickled model of your choosing to get y value revenue predictions per transaction\n",
    "revenue_test_predicted = prediction_from_pickle_model(X_test, 'model_pickles/random_forest_v3_n_estimators_5.pkl')\n",
    "\n",
    "#may need to fill NaNs with zeros if there are NaNs in output (not sure how it will handle things)\n",
    "\n",
    "#create a new dataframe with fullVisitorId and PredictedTransactionRevenue\n",
    "df_visitor_revenue_predicted = pd.DataFrame({\"fullVisitorId\":fullVisitorId_test, \"PredictedTransactionRevenue\": revenue_test_predicted})\n",
    "\n",
    "#competition wants the total revenue by customer, so need to groupby the fullVisitorId and sum all transaction revenue\n",
    "#this will start our final submission df\n",
    "df_submission = df_visitor_revenue_predicted.groupby(\"fullVisitorId\")[\"PredictedTransactionRevenue\"].sum().reset_index()\n",
    "\n",
    "#competition also requires us to take the log of the total revenue by customer for its final metrics, so map that\n",
    "#believe they intend for us to take the log of the total revenue plus 1 (which np.log1p does for us) because \n",
    "#otherwise all the 0 revenues would go to -infinity and mess everything up\n",
    "df_submission[\"PredictedLogRevenue\"] = df_submission[\"PredictedTransactionRevenue\"].map(lambda x: np.log1p(x))\n",
    "\n",
    "#drop the intermediary column of sum of transacation revenues before the log revenue\n",
    "df_submission.drop(\"PredictedTransactionRevenue\", inplace=True)\n",
    "\n",
    "#send the df_submission to csv - use folder kaggle_submissions and use same name as the model (just change .pkl to .csv)\n",
    "df_submission.to_csv(\"kaggle_submissions/random_forest_v3_n_estimators_5.csv\", index=False)\n",
    "print('Results saved to kaggle_submissions folder.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.815511557963774"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log1p(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test[pred_test<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
